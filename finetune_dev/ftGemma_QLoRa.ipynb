{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4256be8-7615-47e3-955a-5b95bb113a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"transformers>=4.44\" \"trl>=0.9.6\" \"peft>=0.12.0\" \\\n",
    "                 \"bitsandbytes>=0.43\" \"accelerate>=0.34\" \\\n",
    "                 \"datasets>=2.20.0\" sentencepiece \"protobuf<5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1aafbbf-bf65-4fd5-9647-fc28bd401702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_MODEL  = \"google/gemma-3-4b-it\"\n",
    "SRC_JSON    = \"/workspace/QAs_Hukumonline_Train.json\"\n",
    "DATA_DIR    = \"/workspace/data\"\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "CHAT_JSONL  = f\"{DATA_DIR}/train_chat.jsonl\"\n",
    "\n",
    "OUT_DIR     = \"/workspace/gemma3_qlora_hukum\"\n",
    "MERGED_DIR  = \"/workspace/gemma3_qlora_merged\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "# Training params\n",
    "MAX_SEQ_LEN    = 4096    # we will pass this to trainer, not SFTConfig\n",
    "EPOCHS         = 2\n",
    "BATCH_SIZE     = 1\n",
    "GRAD_ACC       = 8\n",
    "LORA_R         = 16\n",
    "LORA_ALPHA     = 32\n",
    "LORA_DROPOUT   = 0.05\n",
    "LR             = 2e-4\n",
    "WARMUP_RATIO   = 0.03\n",
    "LOG_STEPS      = 20\n",
    "SAVE_STEPS     = 500\n",
    "EVAL_RATIO     = 0.02     # 0 means no eval split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e44d7524-3bbb-4496-a974-dfd660e2c473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total asli: 384 | Bersih+dedup: 383\n",
      "Saved chat JSONL → /workspace/data/train_chat.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, random, re, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def norm_txt(x: str) -> str:\n",
    "    x = x.strip()\n",
    "    x = unicodedata.normalize(\"NFKC\", x)\n",
    "    x = re.sub(r\"[ \\t]+\", \" \", x)       # collapse spaces/tabs\n",
    "    x = re.sub(r\"\\n{3,}\", \"\\n\\n\", x)    # collapse excessive newlines\n",
    "    return x\n",
    "\n",
    "# 1) Load\n",
    "with open(SRC_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = json.load(f)\n",
    "assert isinstance(rows, list) and all(isinstance(r, dict) for r in rows), \"Expect a list of dicts.\"\n",
    "\n",
    "# 2) Clean & filter\n",
    "clean = []\n",
    "for r in rows:\n",
    "    instr = norm_txt(r.get(\"instruction\", \"\"))\n",
    "    resp  = norm_txt(r.get(\"response\", \"\"))\n",
    "    if not instr or not resp:\n",
    "        continue\n",
    "    if len(instr.split()) < 3 or len(resp.split()) < 5:\n",
    "        continue\n",
    "    clean.append({\"instruction\": instr, \"response\": resp})\n",
    "\n",
    "# 3) Deduplicate by (instruction, response)\n",
    "seen = set(); dedup = []\n",
    "for r in clean:\n",
    "    k = (r[\"instruction\"], r[\"response\"])\n",
    "    if k in seen: \n",
    "        continue\n",
    "    seen.add(k); dedup.append(r)\n",
    "\n",
    "# 4) Export to messages[] JSONL (no split here)\n",
    "def write_jsonl(path, records):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps({\n",
    "                \"messages\": [\n",
    "                    {\"role\":\"system\",\"content\":\"Anda adalah asisten hukum yang akurat, padat, dan mengutip dasar hukum bila relevan.\"},\n",
    "                    {\"role\":\"user\",\"content\": r[\"instruction\"]},\n",
    "                    {\"role\":\"assistant\",\"content\": r[\"response\"]}\n",
    "                ]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "write_jsonl(CHAT_JSONL, dedup)\n",
    "\n",
    "print(f\"Total asli: {len(rows)} | Bersih+dedup: {len(dedup)}\")\n",
    "print(\"Saved chat JSONL →\", CHAT_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5a30433-eadd-46c9-b294-baf9d64388c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.0\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "print(trl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c81396c2-591d-4fd4-ae0e-ea78ec97fa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]\n",
      "Tokenizing train dataset: 100%|██████████| 383/383 [00:01<00:00, 250.83 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 383/383 [00:00<00:00, 8878.13 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 20:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.204500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter saved to: /workspace/gemma3_qlora_hukum\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — QLoRA training (OOM-safe on A40, TRL 0.24.0)\n",
    "import os, torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "# --- dial down sequence length first to avoid OOM ---\n",
    "MAX_LEN_TRAIN = min(MAX_SEQ_LEN, 3072)  # start safer; try 4096 later\n",
    "\n",
    "# 4-bit quantization (use fp16 compute to save VRAM)\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # <- was bfloat16; fp16 saves memory\n",
    ")\n",
    "\n",
    "# Tokenizer (do NOT pass to trainer)\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if getattr(tok, \"chat_template\", None) is None:\n",
    "    tok.padding_side = \"right\"\n",
    "\n",
    "# Dataset: messages[]\n",
    "ds = load_dataset(\"json\", data_files=CHAT_JSONL, split=\"train\")\n",
    "def only_messages(e): return {\"messages\": e[\"messages\"]}\n",
    "ds = ds.map(only_messages, remove_columns=[c for c in ds.column_names if c != \"messages\"])\n",
    "\n",
    "# Turn off eval to avoid evaluation-time memory spikes\n",
    "train_ds, eval_ds = ds, None\n",
    "_eval_strategy = \"no\"\n",
    "\n",
    "# LoRA config\n",
    "peft_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "\n",
    "# TRL 0.24.0 — put max_length, packing, and model_init_kwargs in SFTConfig\n",
    "cfg = SFTConfig(\n",
    "    output_dir=OUT_DIR,\n",
    "    bf16=True,                           # activations in bf16 (safe on A40)\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # Data/packing controls\n",
    "    max_length=MAX_LEN_TRAIN,            # <- control sequence length here\n",
    "    packing=False,\n",
    "\n",
    "    # Forwarded to AutoModelForCausalLM.from_pretrained(...)\n",
    "    model_init_kwargs={\n",
    "        \"quantization_config\": bnb,\n",
    "        \"torch_dtype\": torch.bfloat16,   # model weights bfloat16 on load (ok)\n",
    "        \"device_map\": \"auto\",\n",
    "        \"attn_implementation\": \"sdpa\",   # memory-friendlier attention\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "    },\n",
    "\n",
    "    # No eval to reduce VRAM\n",
    "    eval_strategy=_eval_strategy,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=BASE_MODEL,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=peft_cfg,\n",
    "    args=cfg,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "print(\"LoRA adapter saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e05307ce-1832-4921-b27d-5803ec303dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "import torch, os\n",
    "\n",
    "BASE_MODEL = \"google/gemma-3-4b-it\"\n",
    "OUT_DIR    = \"/workspace/gemma3_qlora_hukum\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, OUT_DIR)\n",
    "\n",
    "# Avoid pad/eos mismatch warnings during generation\n",
    "pad_id = tok.pad_token_id if tok.pad_token_id is not None else tok.eos_token_id\n",
    "tok.pad_token = tok.eos_token if tok.pad_token is None else tok.pad_token\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdd7266d-8106-40e0-9759-83d426b6bbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Q1 ---\n",
      "Pasal 1320 KUHPerdata mengatur tentang kewajiban membayar ganti kerugian kepada orang yang dirugikan akibat perbuatan melawan hukum.\n",
      "\n",
      "Penjelasan Pasal 1320 KUHPerdata adalah sebagai berikut:\n",
      "\n",
      "Pasal 1320\n",
      "\n",
      "Barang siapa dengan sengaja atau karena kelalaiannya menimbulkan kerugian pada orang lain, wajib mengganti kerugian tersebut.\n",
      "\n",
      "Pasal 1320 ayat (1) KUHPerdata mengatur tentang kewajiban membayar ganti kerugian kepada orang yang dirugikan akibat perbuatan melawan hukum.\n",
      "\n",
      "Pasal 1320 ayat (2) KUHPerdata mengatur bahwa kerugian yang harus diganti adalah kerugian yang timbul akibat perbuatan yang melawan hukum.\n",
      "\n",
      "Pasal 1320 ayat (3) KUHPerdata mengatur bahwa kerugian yang harus diganti adalah kerugian yang timbul akibat perbuatan yang melawan hukum.\n",
      "\n",
      "Pasal 1320 ayat (4) KUHPerdata mengatur bahwa kerugian yang harus diganti adalah kerugian yang timbul akibat perbuatan yang melawan hukum.\n",
      "\n",
      "Pasal 1320 ayat (5) KUHPerdata mengatur bahwa kerugian yang harus dig\n",
      "\n",
      "\n",
      "--- Q2 ---\n",
      "Wanprestasi adalah suatu perbuatan yang melanggar suatu perjanjian. Sedangkan perbuatan melawan hukum adalah suatu perbuatan yang bertentangan dengan norma kesusilaan, norma ketertiban umum, atau norma hukum.\n",
      "\n",
      "Perbedaan wanprestasi dan perbuatan melawan hukum dapat dilihat dari unsur-unsur dan akibat hukumnya.\n",
      "\n",
      "Penjelasan lebih lanjut dapat Anda baca ulasan di bawah ini.\n",
      "\n",
      "Terima kasih atas pertanyaan Anda.\n",
      "\n",
      "19 Jul, 2024\n",
      "\n",
      "12 Jul, 2024\n",
      "\n",
      "21 Jun, 2024\n",
      "\n",
      "10 Jun, 2024\n",
      "\n",
      "Wanprestasi adalah suatu perbuatan yang melanggar suatu perjanjian. Sedangkan perbuatan melawan hukum adalah suatu perbuatan yang bertentangan dengan norma kesusilaan, norma ketertiban umum, atau norma hukum.\n",
      "\n",
      "Perbedaan wanprestasi dan perbuatan melawan hukum dapat dilihat dari unsur-unsur dan akibat hukumnya.\n",
      "\n",
      "Unsur-unsur wanprestasi adalah sebagai berikut:\n",
      "\n",
      "Sedangkan unsur-unsur perbuatan melawan hukum adalah sebagai berikut:\n",
      "\n",
      "Berdasarkan perbedaan unsur-unsur tersebut, dapat disimpulkan bahwa wanprestasi adalah perbuatan yang melanggar suatu perjanjian, sedangkan\n",
      "\n",
      "\n",
      "--- Q3 ---\n",
      "Perjanjian adalah suatu kesepakatan antara dua orang atau lebih yang mengikatkan hak dan kewajiban masing-masing pihak.\n",
      "\n",
      "MenurutPasal 1360KUH Perdata, perjanjian adalah kesepakatan antara dua orang atau lebih yang mengikatkan hak dan kewajiban masing-masing pihak.\n",
      "\n",
      "Kemudian, menurutPasal 1331 KUH Perdata, perjanjian adalah kesepakatan antara dua orang atau lebih yang mengikatkan hak dan kewajiban masing-masing pihak.\n",
      "\n",
      "Perjanjian sah apabila memenuhi syarat-syarat sebagai berikut:\n",
      "\n",
      "Pasal 1332 KUH Perdata\n",
      "\n",
      "Pasal 1333 KUH Perdata\n",
      "\n",
      "Pasal 1334 KUH Perdata\n",
      "\n",
      "Pasal 1335 KUH Perdata\n",
      "\n",
      "Pasal 1336 KUH Perdata\n",
      "\n",
      "Pasal 1337 KUH Perdata\n",
      "\n",
      "Pasal 1338 KUH Perdata\n",
      "\n",
      "Pasal 1339 KUH Perdata\n",
      "\n",
      "Pasal 1340 KUH Perdata\n",
      "\n",
      "Pasal 1341 KUH Perdata\n",
      "\n",
      "Pasal 1342 KUH Perdata\n",
      "\n",
      "Pas\n",
      "\n",
      "\n",
      "Approx word lengths: [139, 145, 118]\n",
      "Contains legal terms: [True, False, True]  → score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import re, numpy as np\n",
    "\n",
    "prompts = [\n",
    "    \"User: Ringkas unsur Pasal 1320 KUHPerdata.\\nAssistant:\",\n",
    "    \"User: Jelaskan perbedaan wanprestasi dan perbuatan melawan hukum beserta dasar hukumnya.\\nAssistant:\",\n",
    "    \"User: Apa syarat sahnya perjanjian menurut hukum Indonesia? Cantumkan pasal terkait.\\nAssistant:\",\n",
    "]\n",
    "\n",
    "def gen(p):\n",
    "    return pipe(p, temperature=0.2, do_sample=False, max_new_tokens=256)[0][\"generated_text\"]\n",
    "\n",
    "outs = [gen(p) for p in prompts]\n",
    "for i, o in enumerate(outs, 1):\n",
    "    text = o.split(\"Assistant:\", 1)[-1]\n",
    "    print(f\"\\n--- Q{i} ---\\n{text.strip()}\\n\")\n",
    "\n",
    "# quick-and-dirty “sanity” metrics\n",
    "lens = [len(o.split()) for o in outs]\n",
    "has_legal_terms = [bool(re.search(r\"\\b(Pasal|UU|KUHP|KUHPerdata|Perma|Permen|Putusan)\\b\", o, flags=re.I)) for o in outs]\n",
    "print(\"\\nApprox word lengths:\", lens)\n",
    "print(\"Contains legal terms:\", has_legal_terms, \" → score:\", np.mean(has_legal_terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a32a05b-1ede-48aa-b67f-17f519313a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged full model saved to: /workspace/gemma3_qlora_merged\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch, os\n",
    "\n",
    "BASE_MODEL = \"google/gemma-3-4b-it\"\n",
    "OUT_DIR    = \"/workspace/gemma3_qlora_hukum\"\n",
    "MERGED_DIR = \"/workspace/gemma3_qlora_merged\"\n",
    "\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.bfloat16, device_map=\"cpu\"\n",
    ")\n",
    "merged = PeftModel.from_pretrained(base, OUT_DIR).merge_and_unload()\n",
    "\n",
    "merged.save_pretrained(MERGED_DIR)\n",
    "tok.save_pretrained(MERGED_DIR)\n",
    "print(\"Merged full model saved to:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd42028-32fe-49de-8bd6-bb5dabcb0823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-ftQLoRa)",
   "language": "python",
   "name": "venv-ftqlora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
