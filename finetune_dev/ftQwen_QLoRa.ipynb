{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56361172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, re, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88da6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model ----\n",
    "BASE_MODEL  = \"Qwen/Qwen2.5-3B-Instruct\"   # Qwen 2.5 3B Instruct\n",
    "\n",
    "# ---- Dataset paths ----\n",
    "SRC_JSON    = \"/workspace/QAs_Hukumonline_Train.json\"  # your original JSON\n",
    "DATA_DIR    = \"/workspace/data\"\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "CHAT_JSONL  = f\"{DATA_DIR}/train_chat_qwen.jsonl\"\n",
    "\n",
    "# ---- Output dirs ----\n",
    "OUT_DIR     = \"/workspace/qwen25_3b_qlora_hukum\"\n",
    "MERGED_DIR  = \"/workspace/qwen25_3b_qlora_merged\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Training params ----\n",
    "MAX_SEQ_LEN    = 4096\n",
    "EPOCHS         = 2\n",
    "BATCH_SIZE     = 1\n",
    "GRAD_ACC       = 8\n",
    "LORA_R         = 16\n",
    "LORA_ALPHA     = 32\n",
    "LORA_DROPOUT   = 0.05\n",
    "LR             = 2e-4\n",
    "WARMUP_RATIO   = 0.03\n",
    "LOG_STEPS      = 20\n",
    "SAVE_STEPS     = 500\n",
    "EVAL_RATIO     = 0.0   # keep 0.0 for OOM-safe (no eval)\n",
    "\n",
    "# OOM safety on GPU pods\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "random.seed(42)\n",
    "\n",
    "print(\"BASE_MODEL:\", BASE_MODEL)\n",
    "print(\"SRC_JSON  :\", SRC_JSON)\n",
    "print(\"CHAT_JSONL:\", CHAT_JSONL)\n",
    "print(\"OUT_DIR   :\", OUT_DIR)\n",
    "print(\"MERGED_DIR:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf36ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_txt(x: str) -> str:\n",
    "    x = (x or \"\").strip()\n",
    "    x = unicodedata.normalize(\"NFKC\", x)\n",
    "    x = re.sub(r\"[ \\t]+\", \" \", x)       # collapse spaces/tabs\n",
    "    x = re.sub(r\"\\n{3,}\", \"\\n\\n\", x)    # collapse excessive newlines\n",
    "    return x\n",
    "\n",
    "with open(SRC_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = json.load(f)\n",
    "\n",
    "assert isinstance(rows, list) and all(isinstance(r, dict) for r in rows), \"Expect a list of dicts.\"\n",
    "\n",
    "clean = []\n",
    "for r in rows:\n",
    "    instr = norm_txt(r.get(\"instruction\", \"\"))\n",
    "    resp  = norm_txt(r.get(\"response\", \"\"))\n",
    "    if not instr or not resp:\n",
    "        continue\n",
    "    if len(instr.split()) < 3 or len(resp.split()) < 5:\n",
    "        continue\n",
    "    clean.append({\"instruction\": instr, \"response\": resp})\n",
    "\n",
    "# Dedup\n",
    "seen = set()\n",
    "dedup = []\n",
    "for r in clean:\n",
    "    k = (r[\"instruction\"], r[\"response\"])\n",
    "    if k in seen:\n",
    "        continue\n",
    "    seen.add(k)\n",
    "    dedup.append(r)\n",
    "\n",
    "def write_jsonl(path, records):\n",
    "    sys_prompt = \"Anda adalah asisten hukum yang akurat, padat, dan mengutip dasar hukum bila relevan.\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            obj = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": r[\"instruction\"]},\n",
    "                    {\"role\": \"assistant\", \"content\": r[\"response\"]},\n",
    "                ]\n",
    "            }\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(CHAT_JSONL, dedup)\n",
    "\n",
    "print(f\"Total asli: {len(rows)} | Bersih+dedup: {len(dedup)}\")\n",
    "print(\"Saved chat JSONL â†’\", CHAT_JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl\n",
    "print(\"trl:\", trl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefdc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Sequence length safety dial (naikkan kalau aman)\n",
    "MAX_LEN_TRAIN = min(MAX_SEQ_LEN, 3072)  # start safer; try 4096 later\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # fp16 saves memory\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "# Dataset: messages[]\n",
    "ds = load_dataset(\"json\", data_files=CHAT_JSONL, split=\"train\")\n",
    "ds = ds.map(lambda e: {\"messages\": e[\"messages\"]},\n",
    "            remove_columns=[c for c in ds.column_names if c != \"messages\"])\n",
    "\n",
    "# Turn off eval to reduce VRAM spikes\n",
    "train_ds, eval_ds = ds, None\n",
    "_eval_strategy = \"no\"\n",
    "\n",
    "# LoRA config (Qwen2.5 uses LLaMA-like projection names)\n",
    "peft_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=OUT_DIR,\n",
    "    bf16=True,                           # A40 supports bf16\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # sequence\n",
    "    max_length=MAX_LEN_TRAIN,\n",
    "    packing=False,\n",
    "\n",
    "    # forwarded to AutoModelForCausalLM.from_pretrained(...)\n",
    "    model_init_kwargs={\n",
    "        \"quantization_config\": bnb,\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"device_map\": \"auto\",\n",
    "        \"attn_implementation\": \"sdpa\",\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"trust_remote_code\": True,\n",
    "    },\n",
    "\n",
    "    eval_strategy=_eval_strategy,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=BASE_MODEL,           # TRL will load via model_init_kwargs\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=peft_cfg,\n",
    "    args=cfg,\n",
    "    tokenizer=tok,              # ensure chat_template applied properly\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "print(\"LoRA adapter saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd59808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, OUT_DIR)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "def gen_chat(user_text: str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Anda adalah asisten hukum yang akurat, padat, dan mengutip dasar hukum bila relevan.\"},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    out = pipe(prompt, temperature=0.2, do_sample=False, max_new_tokens=256)[0][\"generated_text\"]\n",
    "    return out[len(prompt):].strip()\n",
    "\n",
    "tests = [\n",
    "    \"Ringkas unsur Pasal 1320 KUHPerdata.\",\n",
    "    \"Jelaskan perbedaan wanprestasi dan perbuatan melawan hukum beserta dasar hukumnya.\",\n",
    "    \"Apa syarat sahnya perjanjian menurut hukum Indonesia? Cantumkan pasal terkait.\",\n",
    "]\n",
    "for i, t in enumerate(tests, 1):\n",
    "    print(f\"\\n--- Q{i} ---\\n{gen_chat(t)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c366a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "base_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "merged = PeftModel.from_pretrained(base_cpu, OUT_DIR).merge_and_unload()\n",
    "\n",
    "merged.save_pretrained(MERGED_DIR)\n",
    "tok.save_pretrained(MERGED_DIR)\n",
    "print(\"Merged full model saved to:\", MERGED_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
