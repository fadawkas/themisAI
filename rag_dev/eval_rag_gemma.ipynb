{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3fc1d-f6e4-41b6-82a7-08b5e89d3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_similarity\n",
    "from ragas.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# LangChain OpenAI client pointed to your local vLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "try:\n",
    "    from ragas.llms.langchain import LangchainLLMWrapper  # newer path\n",
    "except Exception:\n",
    "    from ragas.llms import LangchainLLMWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4745bb-99c4-41a0-a2d3-c2000188f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.setdefault(\"OPENAI_BASE_URL\", \"http://127.0.0.1:8002/v1\")\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"not-needed\")  # set real key only if vLLM started with --api-key\n",
    "\n",
    "VLLM_URL   = os.getenv(\"OPENAI_BASE_URL\")\n",
    "MODEL_NAME = os.getenv(\"VLLM_MODEL\", \"google/gemma-3-4b-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62937b21-6f17-4c3b-8bb3-14a3dfdfd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llm = ChatOpenAI(\n",
    "    model=MODEL_NAME,\n",
    "    base_url=VLLM_URL,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    temperature=0.2,\n",
    ")\n",
    "llm = LangchainLLMWrapper(judge_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7ac0e-67a4-4d45-a86f-4cafc3882796",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# â”€â”€ Load generations CSV (requires columns: question, answer, ground_truth) â”€â”€â”€â”€\n",
    "csv_path = \"/workspace/gemma_rag_generations.csv\"  # <- or \"/workspace/gemma_generations.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "assert {\"question\",\"answer\",\"ground_truth\"}.issubset(df.columns), \\\n",
    "    f\"Missing columns. Have: {df.columns.tolist()}\"\n",
    "\n",
    "# Build RAGAS dataset\n",
    "ragas_ds = Dataset.from_pandas(df[[\"question\",\"answer\",\"ground_truth\"]])\n",
    "\n",
    "# â”€â”€ Evaluate (answer_similarity only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "executor = evaluate(\n",
    "    ragas_ds,\n",
    "    metrics=[answer_similarity],\n",
    "    llm=llm,                # not strictly required for this metric, but safe\n",
    "    embeddings=emb,\n",
    "    show_progress=True,\n",
    "    return_executor=True,\n",
    "    # Tip: if your vLLM struggles with concurrency, add: batch_size=2\n",
    ")\n",
    "\n",
    "# Collect results (list of floats)\n",
    "try:\n",
    "    raw = executor.results() if hasattr(executor, \"results\") else executor\n",
    "except TypeError:\n",
    "    raw = executor.results  # some versions expose a property\n",
    "\n",
    "if not isinstance(raw, list):\n",
    "    raise RuntimeError(f\"Expected a list of floats, got {type(raw)}\")\n",
    "\n",
    "# Per-row and overall\n",
    "per_row_df = pd.DataFrame(raw, columns=[\"answer_similarity_score\"])\n",
    "overall_df = per_row_df.mean().to_frame().T\n",
    "overall_df.columns = [c + \"_mean\" for c in overall_df.columns]\n",
    "\n",
    "# Save\n",
    "overall_out = \"/workspace/gemma_ragas_overall.csv\"\n",
    "perrow_out = \"/workspace/gemma_ragas_per_row.csv\"\n",
    "overall_df.to_csv(overall_out, index=False)\n",
    "per_row_df.to_csv(perrow_out, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", overall_out)\n",
    "print(\" -\", perrow_out)\n",
    "print(\"Preview:\")\n",
    "display(overall_df)\n",
    "display(per_row_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2db96c-5854-4d1e-a670-58abdf961775",
   "metadata": {},
   "outputs": [],
   "source": [
    "`# %%\n",
    "import os, json, pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Try both import paths (supports old/new ragas versions)\n",
    "try:\n",
    "    from ragas.llms.langchain import LangchainLLMWrapper\n",
    "except ModuleNotFoundError:\n",
    "    from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# vllm serve Qwen/Qwen2.5-3B-Instruct --port 8002 --gpu-memory-utilization 0.8 --dtype bfloat16\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"http://127.0.0.1:8002/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"not-needed\"\n",
    "\n",
    "judge_llm = ChatOpenAI(\n",
    "    model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    base_url=os.environ[\"OPENAI_BASE_URL\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    temperature=0.2,\n",
    ")\n",
    "llm = LangchainLLMWrapper(judge_llm)\n",
    "emb = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "csv_path = \"/workspace/gemma_rag_generations.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Loaded CSV with columns:\", df.columns.tolist())\n",
    "\n",
    "def to_contexts_safe(x):\n",
    "    \"\"\"Safely normalize any context/contextS column to List[str].\"\"\"\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(t) for t in x]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # Try JSON list first\n",
    "        try:\n",
    "            v = json.loads(s)\n",
    "            if isinstance(v, list):\n",
    "                return [str(t) for t in v]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Otherwise split by double newlines\n",
    "        parts = [p.strip() for p in s.split(\"\\n\\n\") if p.strip()]\n",
    "        return parts if parts else [s]\n",
    "    return [str(x)]\n",
    "\n",
    "src_col = \"contexts\" if \"contexts\" in df.columns else \"context\"\n",
    "df[\"contexts\"] = df[src_col].apply(to_contexts_safe)\n",
    "\n",
    "cols = [\"question\", \"answer\", \"contexts\"]\n",
    "if \"ground_truth\" in df.columns:\n",
    "    cols.append(\"ground_truth\")\n",
    "ragas_ds = Dataset.from_pandas(df[cols])\n",
    "print(\"Prepared dataset with\", len(ragas_ds), \"rows\")\n",
    "\n",
    "def metric_mean(res, metric_name: str) -> float:\n",
    "    \"\"\"Handle all RAGAS versions consistently.\"\"\"\n",
    "    import numpy as np\n",
    "    try:\n",
    "        df = res.to_pandas()\n",
    "        if metric_name in df.columns:\n",
    "            return float(df[metric_name].mean())\n",
    "        return float(df.mean(numeric_only=True).iloc[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    if hasattr(res, \"scores\"):\n",
    "        s = res.scores\n",
    "        if isinstance(s, list):\n",
    "            return float(pd.Series(s).mean())\n",
    "        if isinstance(s, dict) and metric_name in s:\n",
    "            return float(pd.Series(s[metric_name]).mean())\n",
    "    try:\n",
    "        vals = res if isinstance(res, list) else res.results()\n",
    "        return float(pd.Series(vals).mean())\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "results = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"\\nðŸ”¹ Evaluating metric: {metric.name}\")\n",
    "    try:\n",
    "        res = evaluate(\n",
    "            dataset=ragas_ds,\n",
    "            metrics=[metric],\n",
    "            llm=llm,\n",
    "            embeddings=emb,\n",
    "            batch_size=2,        # safer for vLLM stability\n",
    "            show_progress=True,\n",
    "        )\n",
    "        m = metric_mean(res, metric.name)\n",
    "        results[metric.name] = m\n",
    "        print(f\"{metric.name} â†’ {m}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {metric.name} due to error:\", e)\n",
    "\n",
    "out_path = \"/workspace/qwen_ragas_metric_means.csv\"\n",
    "pd.DataFrame([results]).to_csv(out_path, index=False)\n",
    "print(\"\\nAll metrics done. Saved mean scores to:\", out_path)\n",
    "print(pd.DataFrame([results]))\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f5d53-14fb-412e-b373-4a6e67e24b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-themisft)",
   "language": "python",
   "name": "venv-themisft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
